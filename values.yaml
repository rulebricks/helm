# =============================================================================
# GLOBAL RULEBRICKS SETTINGS
# These important values are used across multiple components and should be set here
# =============================================================================
global:
  # Domain for ingress and TLS certificate generation
  domain: "<rulebricks-subdomain>.<domain-you-control>"
  # Email for TLS certificate registration (required for cert-manager)
  email: "support@rulebricks.com"
  # License Key for Rulebricks Enterprise (provided in your onboarding space)
  licenseKey: "evaluation"
  # Enable TLS/HTTPS - requires DNS records to be configured first
  # Set to `true` when using external-dns OR after manually creating DNS records
  tlsEnabled: false
  # Enable external-dns integration for automatic DNS record management
  # Works with either: the external-dns subchart below, OR an existing cluster-wide external-dns
  externalDnsEnabled: false
  # SMTP Configuration (Vital for instance access and admin)
  smtp:
    host: "smtp.mailtrap.io"
    port: 2525
    user: "demo-user"
    pass: "demo-password"
    from: "no-reply@rulebricks.com"
    fromName: "Rulebricks"
  # Auth/Database Configuration
  supabase:
    anonKey: "eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyAgCiAgICAicm9sZSI6ICJhbm9uIiwKICAgICJpc3MiOiAic3VwYWJhc2UtZGVtbyIsCiAgICAiaWF0IjogMTY0MTc2OTIwMCwKICAgICJleHAiOiAxNzk5NTM1NjAwCn0.dc_X5iR_VP_qT0zsiyj_I_OZ2T9FtRU2BBNWN8Bu4GE"
    serviceKey: "eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyAgCiAgICAicm9sZSI6ICJzZXJ2aWNlX3JvbGUiLAogICAgImlzcyI6ICJzdXBhYmFzZS1kZW1vIiwKICAgICJpYXQiOiAxNjQxNzY5MjAwLAogICAgImV4cCI6IDE3OTk1MzU2MDAKfQ.DaYlNEoUrrEn2Ig7tqibS-PHK5vgusbcbo7X36XVt4Q"
    # Email Customization (subjects and templates for auth emails)
    emails:
      subjects:
        invite: "Join your team on Rulebricks"
        confirmation: "Confirm Your Email"
        recovery: "Reset Your Password"
        emailChange: "Confirm Email Change"
      templates:
        # HTML templates hosted on S3 (or any public URL)
        invite: "https://prefix-files.s3.us-west-2.amazonaws.com/templates/invite.html"
        confirmation: "https://prefix-files.s3.us-west-2.amazonaws.com/templates/verify.html"
        recovery: "https://prefix-files.s3.us-west-2.amazonaws.com/templates/password_change.html"
        emailChange: "https://prefix-files.s3.us-west-2.amazonaws.com/templates/email_change.html"
    # [Managed Supabase] Leave URL empty if self-hosting
    url: ""
    # [Managed Supabase] Project reference (e.g., "abcdefghijkl")
    projectRef: ""
    # [Managed Supabase] Access token for headless CLI authentication
    # Generate from Supabase Dashboard: Account Settings > Access Tokens
    # Required when using managed Supabase (supabase.enabled=false)
    accessToken: ""
    # [Self-hosted Supabase] JWT signing secret (only needed for self-hosted Supabase)
    jwtSecret: "your-super-secret-jwt-token-with-at-least-32-characters-long"
  # AI Features (Optional)
  # Enables AI-powered rule generation and suggestions
  ai:
    enabled: false
    openaiApiKey: "" # Your OpenAI API key

  # =============================================================================
  # SECRETS CONFIGURATION (Optional)
  # Enterprise users can reference a pre-existing Kubernetes secret instead of
  # specifying sensitive values in this file. When secretRef is set, the chart
  # will use the external secret instead of creating its own.
  #
  # Note: This only affects the Rulebricks application secrets, not self-hosted
  # Supabase internal secrets (JWT, DB password, dashboard) which remain as values.
  # =============================================================================
  secrets:
    # Optional: reference an existing Kubernetes secret for all sensitive values
    # When set, the chart will NOT create its own secret and will use this instead
    # Example: "rulebricks-secrets"
    secretRef: ""
    # Key mappings for the external secret (customize if your secret uses different keys)
    secretRefKeys:
      licenseKey: "LICENSE_KEY"
      smtpUser: "SMTP_USER"
      smtpPass: "SMTP_PASS"
      supabaseAnonKey: "SUPABASE_ANON_KEY"
      supabaseServiceKey: "SUPABASE_SERVICE_KEY"
      supabaseAccessToken: "SUPABASE_ACCESS_TOKEN"
      openaiApiKey: "OPENAI_API_KEY"

# =============================================================================
# RULEBRICKS APPLICATION STACK
# This is the core Rulebricks application stack. It is preconfigured to run as-
# is, but you will likely need to manually tune component resources and
# other configurations to your business's performance requirements.
# We highly recommend you keep Traefik and Kafka internal to the cluster– our
# solver directly depends on their configurations.
# =============================================================================
rulebricks:
  app:
    image:
      repository: index.docker.io/rulebricks/app
      tag: "1.2.8" # https://hub.docker.com/repository/docker/rulebricks/app/tags
      pullPolicy: IfNotPresent

    logging:
      enabled: true
      # Leave empty to auto-discover Kafka from the kafka subchart (recommended)
      # Only set explicitly if using an external Kafka cluster
      kafkaBrokers: ""
      kafkaTopic: "logs"
      # Label to display in App UI – does not change the logging destination
      # See Vector config below to configure log forwarding
      loggingDestination: "Console (stdout)"

  # HPS (High Performance Server)
  # HPS intercepts requests on solution endpoints via Traefik and processes them scalably
  hps:
    enabled: true
    image:
      repository: index.docker.io/rulebricks/hps
      tag: "1.2.8" # also controls the worker image tag
      pullPolicy: Always # HPS image may receive more frequent updates
    replicas: 3 # Minimum recommended
    resources:
      requests:
        cpu: "1000m"
        memory: "1Gi"
      limits:
        cpu: "2000m"
        memory: "2Gi"
    workers:
      enabled: true
      replicas: 4
      keda:
        enabled: true
        minReplicaCount: 4
        maxReplicaCount: 12 # Matched to Kafka partition count
        pollingInterval: 10
        cooldownPeriod: 300
        lagThreshold: 50 # Scale when consumer lag exceeds this threshold
        cpuThreshold: 25

  ingress:
    enabled: true
    className: traefik
    # Host is automatically configured from global.domain
    paths:
      - path: /
        pathType: Prefix

  # Redis is used heavily
  redis:
    resources:
      requests:
        cpu: "200m"
        memory: "256Mi"
      limits:
        cpu: "500m"
        memory: "4Gi"
    persistence:
      enabled: true
      size: "4Gi"
      storageClass: gp3

# Message Queue (Kafka)
kafka:
  enabled: true
  # KRaft mode (no Zookeeper)
  kraft:
    enabled: true
  zookeeper:
    enabled: false
  # Kafka broker configuration
  overrideConfiguration:
    auto.create.topics.enable: "true"
    log.retention.hours: "24"
    default.replication.factor: "1"
    offsets.topic.replication.factor: "1"
    num.partitions: "12" # Must be >= maxReplicaCount for workers to enable parallel consumption
  controller:
    replicaCount: 1
    resources:
      requests:
        cpu: "500m"
        memory: "2Gi"
      limits:
        cpu: "2000m"
        memory: "3Gi"
    persistence:
      enabled: true
      size: 10Gi
      storageClass: gp3
    # Tune heap options for your needs
    heapOpts: "-Xmx1g -Xms1g -XX:+UseZGC -XX:+AlwaysPreTouch"
    extraEnvVars:
      - name: KAFKA_JVM_PERFORMANCE_OPTS
        value: "-XX:MaxDirectMemorySize=256M -Djdk.nio.maxCachedBufferSize=262144"
      - name: KAFKA_CFG_QUEUED_MAX_REQUESTS
        value: "10000"
      - name: KAFKA_CFG_NUM_NETWORK_THREADS
        value: "8"
      - name: KAFKA_CFG_NUM_IO_THREADS
        value: "8"
      - name: KAFKA_CFG_SOCKET_SEND_BUFFER_BYTES
        value: "1048576"
      - name: KAFKA_CFG_SOCKET_RECEIVE_BUFFER_BYTES
        value: "1048576"
      - name: KAFKA_CFG_SOCKET_REQUEST_MAX_BYTES
        value: "209715200"
      - name: KAFKA_CFG_LOG_RETENTION_BYTES
        value: "4294967296"
      - name: KAFKA_CFG_LOG_SEGMENT_BYTES
        value: "1073741824"
      - name: KAFKA_CFG_NUM_REPLICA_FETCHERS
        value: "4"
      - name: KAFKA_CFG_REPLICA_SOCKET_RECEIVE_BUFFER_BYTES
        value: "1048576"
      - name: KAFKA_CFG_LOG_CLEANER_DEDUPE_BUFFER_SIZE
        value: "268435456"
      - name: KAFKA_CFG_LOG_CLEANER_IO_BUFFER_SIZE
        value: "1048576"
      - name: KAFKA_CFG_MAX_IN_FLIGHT_REQUESTS_PER_CONNECTION
        value: "10"
  listeners:
    client:
      protocol: PLAINTEXT
    controller:
      protocol: PLAINTEXT
    interbroker:
      protocol: PLAINTEXT

# Ingress Controller (Traefik)
traefik:
  enabled: true
  ingressClass:
    name: traefik # Must match ingress.className in rulebricks and supabase
  autoscaling:
    enabled: true
    minReplicas: 1
    maxReplicas: 2
  resources:
    requests:
      cpu: "100m"
      memory: "256Mi"
    limits:
      cpu: "1000m"
      memory: "2Gi"
  ports:
    web:
      port: 8000
      exposedPort: 80
    websecure:
      port: 8443
      exposedPort: 443
  persistence:
    enabled: false

# Autoscaling (KEDA)
keda:
  enabled: true
  crds:
    install: false # CRDs are managed in parent chart crds/ directory

# Certificate Management (cert-manager)
cert-manager:
  enabled: true
  installCRDs: false # CRDs are managed in parent chart crds/ directory

# =============================================================================
# DECISION LOGS (Vector)
# Aggregates and forwards rule execution logs to one or more configurable sinks (console, S3, etc.)
# The Kafka broker address is automatically configured via the vector-env-configmap.yaml template.
# =============================================================================
vector:
  enabled: true
  role: "Stateless-Aggregator"
  replicas: 2
  resources:
    requests:
      cpu: "50m"
      memory: "128Mi"
    limits:
      cpu: "200m"
      memory: "256Mi"
  service:
    enabled: true
    ports:
      - name: api
        port: 8686
        protocol: TCP
        targetPort: 8686
  # Load KAFKA_BOOTSTRAP_SERVERS from the templated ConfigMap (see templates/vector-env-configmap.yaml)
  # Automatically constructs the correct internal Kafka service name based on your Helm release name.
  env:
    - name: KAFKA_BOOTSTRAP_SERVERS
      valueFrom:
        configMapKeyRef:
          name: vector-kafka-env
          key: KAFKA_BOOTSTRAP_SERVERS
  customConfig:
    sources:
      kafka:
        type: kafka
        # KAFKA_BOOTSTRAP_SERVERS is set automatically via env from the vector-env ConfigMap
        # Fallback value is only used if the env var is somehow not set
        bootstrap_servers: "${KAFKA_BOOTSTRAP_SERVERS:-rulebricks-kafka:9092}"
        topics:
          - logs
        group_id: vector-consumers
        auto_offset_reset: latest
    sinks:
      console:
        type: console
        inputs:
          - kafka
        encoding:
          codec: json
      # ==== S3 Sink Example ====
      # To send logs to S3, uncomment and configure the following:
      # Requires IRSA (IAM Roles for Service Accounts) - see README.md for setup
      #
      # s3:
      #   type: aws_s3
      #   inputs:
      #     - kafka
      #   bucket: "your-logs-bucket"
      #   region: "us-east-1"
      #   key_prefix: "rulebricks/logs/%Y/%m/%d/"
      #   compression: gzip
      #   encoding:
      #     codec: json
      #   # Authentication via IRSA (recommended) - no credentials needed in config
      #   # Vector will use the IAM role attached to its service account
      #   #
      #   # Alternative: use static credentials (not recommended for production)
      #   # auth:
      #   #   access_key_id: "${AWS_ACCESS_KEY_ID}"
      #   #   secret_access_key: "${AWS_SECRET_ACCESS_KEY}"

# =============================================================================
# SELF-HOSTED DATABASE SETTINGS (Supabase)
# Set enabled: false to use an external Supabase instance instead
# =============================================================================
supabase:
  enabled: true
  secret:
    # SMTP credentials are sourced from global.smtp.user and global.smtp.pass
    # JWT keys are sourced from global.supabase.* (anonKey, serviceKey, jwtSecret)
    db:
      username: "postgres"
      password: "postgres-password-change-me"
      database: "postgres"
    dashboard:
      username: "supabase"
      password: "dashboard-password-change-me"
  # Database resource and storage configuration (for self-hosted)
  db:
    resources:
      requests:
        cpu: "500m"
        memory: "1Gi"
    persistence:
      enabled: true
      size: 10Gi
      storageClassName: gp3
  # Kong API Gateway ingress configuration
  kong:
    ingress:
      enabled: true
      className: traefik
      annotations: {}

# =============================================================================
# MONITORING & ALERTING
# Controls cluster metrics and alerting via Prometheus (optional)
# =============================================================================
monitoring:
  enabled: false
kube-prometheus-stack:
  alertmanager:
    enabled: false
  grafana:
    enabled: false
  prometheus:
    prometheusSpec:
      retention: 30d
      storageSpec:
        volumeClaimTemplate:
          spec:
            storageClassName: gp3
            accessModes:
              - ReadWriteOnce
            resources:
              requests:
                storage: 50Gi
      remoteWrite: []

# =============================================================================
# STORAGE CLASS (AWS EBS gp3)
# Creates a gp3 StorageClass if it doesn't exist
# =============================================================================
storageClass:
  create: true
  name: gp3
  provisioner: ebs.csi.aws.com
  type: gp3
  fsType: ext4
  reclaimPolicy: Delete
  volumeBindingMode: WaitForFirstConsumer
  allowVolumeExpansion: true

# =============================================================================
# AUTOMATIC DNS MANAGEMENT (External-DNS)
# Deploys external-dns to automatically create DNS records for your domain.
# This enables single-phase installation with TLS from the start.
#
# Two usage modes:
#   1. Deploy external-dns with this chart:
#      Set external-dns.enabled=true AND global.externalDnsEnabled=true
#
#   2. Use existing cluster-wide external-dns:
#      Set only global.externalDnsEnabled=true (external-dns.enabled=false)
#      Your existing external-dns will pick up the ingress annotations
# =============================================================================
external-dns:
  enabled: false
  # Provider: route53, cloudflare, google, azure
  provider: route53
  # Sources to watch for DNS records (ingress is the default)
  sources:
    - ingress
  # Domain filter - restricts external-dns to your domain only
  domainFilters: []
  # Policy: sync (create & delete), upsert-only (create only)
  policy: upsert-only
  # --- Provider-specific configuration ---
  # For AWS Route53: uses IRSA by default (recommended)
  # Ensure your EKS cluster has IRSA configured and the service account
  # has the appropriate IAM role attached.
  #
  # For Cloudflare: set env variables
  # extraEnvVars:
  #   - name: CF_API_TOKEN
  #     valueFrom:
  #       secretKeyRef:
  #         name: cloudflare-api-token
  #         key: api-token
  #
  # For Google Cloud DNS: uses workload identity by default
  # google:
  #   project: "your-gcp-project"
  #
  # For Azure DNS:
  # azure:
  #   resourceGroup: "your-resource-group"
  #   subscriptionId: "your-subscription-id"
